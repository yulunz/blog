<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width" />

    <title>Two Interesting, Short Probability Questions | The Responsible Adult</title>

    <link href="static/blog.css" rel="stylesheet" />
    <link rel="shortcut icon" href="static/favicon.ico" type="image/x-icon">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,100..900;1,100..900&family=Nunito+Sans:ital,opsz,wght@0,6..12,200..1000;1,6..12,200..1000&display=swap" rel="stylesheet">    

    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body>
    <div id="main" class="">
<h1>Two Interesting, Short Probability Questions</h1>
<i>2019-02-28</i>


<ol>
<li>
    If you have \(N\) coins, and \(x\) of them are special in the sense that they have head both front and back. And you pick one up randomly and flip it \(k\) times and get 10 heads, what is the probability that the coin you picked is one of the special ones?

    <br><br><i>Solution:</i> Say \(A\) is the event that you get 10 heads, \(B\) is the event you picked one of the special coins. Then, by Law of Total Probability,

    $$P(A) = P(A|B)\cdot P(B) + P(A|B^{C})\cdot P(B^{C}) = 1\cdot\frac{x}{N} + \big(\frac{1}{2}\big)^{k}\cdot\frac{N-x}{N}$$

    Now, from the Bayes Theorem,

    $$P(B | A) = \frac{P(A, B)}{P(A)} = \frac{\cfrac{1}{N}}{1\cdot\cfrac{x}{N} + \big(\cfrac{1}{2}\big)^{k}\cdot\cfrac{N-x}{N}}=\frac{1}{x + \cfrac{N-1}{2^k}}$$

    Say for \(N = 100\), \(x = 10\), and \(k = 2\), the probability will be about \(2.9\%\).
</li>
<br><br>
<li>
    Say you have \(N\) ropes, or lines, or spaghetti. Each time you pick two ends and tie them up together. What is the expected number of loops?

    <br><br><i>Solution:</i> This one is for bored people. So for each round, after you picked up two ends and connect them, they either form a circle, or they do not. In either case the total number of ropes available decreases by \(1\). Say in a round we have \(i\) ropes left, then let \(R_i\) be a Bernoulli r.v. denoting if  a loop is formed, by Law of Total Probability, 

    $$\mathbb{E}(R_i) = \sum_{j} P(\mbox{picked up rope }j) \cdot P(\mbox{forming a loop})= \sum_{j=1}^{2i} \frac{1}{2i}\cdot\frac{1}{2i - 1} = \frac{1}{2i-1}$$

    Summing over all \(N\) rounds, and let \(R\) be the r.v. of total number of loops formed.

    $$\mathbb{E}(R) = \mathbb{E}\Big(\sum_{i} R_i\Big)=\sum_i \mathbb{E}(R_i) = \sum_{i=1}^{N}\frac{1}{2i-1}$$

    Second step is true since expectation is a linear operator.

    So for \(N=100\), this is about

    <code>

    py> expected_n_of_loops = lambda N: sum(1.0/(2*i - 1) for i in range(1,N))
    py> expected_n_of_loops(100)
    # 3.2793170636734916</code>
    <p>Note the generator expression, without of a list or a tuple, in the <code>sum()</code> function. </p>
</li>

</ol>

    <span id="copyright">Â© The Responsible Adult 2017 - 2025</span>
    </div>
  </body>
</html>
